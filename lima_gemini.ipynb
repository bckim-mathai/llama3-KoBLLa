{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoLIMA(MathAI) and BiLIMA\n",
    "\n",
    "KoLIMA(MathAI) is a Korean translation of the [LIMA: Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf), created using Google's Gemini Pro 1.5.\n",
    "\n",
    "While the [taeshahn/ko-lima](https://huggingface.co/datasets/taeshahn/ko-lima) dataset already exists, our KoLIMA(MathAI) dataset differs significantly in its use of Gemini Pro 1.5 for translation instead of the [DeepL API](https://developers.deepl.com/docs).\n",
    "Furthermore, our dataset features user queries written in informal Korean (banmal, 반말) and assistant responses in formal Korean (jondaetmal, 존댓말).\n",
    "\n",
    "BiLIMA is a bilingual LIMA dataset with two modes: `en_ko` and `ko_en`.\n",
    "- `en_ko`: the user's query is given in English and the assistant's answer is given in Korean.\n",
    "- `ko_en`: the user's query is given in Korean and the assistant's answer is given in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIMA\n",
    "\n",
    "https://huggingface.co/datasets/GAIR/lima\n",
    "\n",
    "### License\n",
    "\n",
    "If the source data of LIMA has a stricter license than CC BY-NC-SA, the LIMA dataset follows the same. Otherwise, it follows the CC BY-NC-SA license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"GAIR/lima\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate KoLIMA(MathAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Pro 1.5\n",
    "\n",
    "You'll need a [Google Gemini API Key](https://aistudio.google.com/app/apikey) to run the following script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = ''\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_NONE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "system_instruction = \"Translate the following English conversation into modern and natural Korean, following four rules:\\n* For terminology, you can use the English word.\\n* If there is no proper Korean word, then you can use the English word.\\n* Each conversation turn is separated by [sep].\\n* Translate the user's query (odd-numbered turns) into polite and friendly informal Korean (반말), and translate the assistant's responses (even-numbered turns) into formal Korean (존댓말).\\n\"\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\",\n",
    "                              generation_config=generation_config,\n",
    "                              system_instruction=system_instruction,\n",
    "                              safety_settings=safety_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepair Dataset\n",
    "\n",
    "Each conversation turn is separated by `[sep]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df = pd.DataFrame([{\n",
    "  'conversations': str('\\n[sep]\\n'.join(data['conversations'])),\n",
    "  'source': str(data['source']),\n",
    "} for data in dataset['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_df.loc[0,'conversations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ko_lima(\n",
    "    lima_df:pd.DataFrame,\n",
    "    file_path:str='./data/ko_lima.csv',\n",
    "    resume:bool=False,\n",
    "    first_sleep=10,\n",
    "    second_sleep=20,\n",
    "  ) -> None:\n",
    "  if resume:\n",
    "    train_dataset_ko_df = pd.read_csv(file_path)\n",
    "  else:\n",
    "    train_dataset_ko_df = lima_df.copy()\n",
    "    train_dataset_ko_df.loc[:, 'korean_conversations'] = None\n",
    "    \n",
    "  idx = (train_dataset_ko_df.loc[:, 'korean_conversations'].isna())\n",
    "\n",
    "  for i, (text, _, _) in tqdm(train_dataset_df[idx].iterrows(), total=len(train_dataset_df[idx])):\n",
    "    print(f\"({i})\\n{text}\")\n",
    "    try:\n",
    "      convo = model.start_chat(history=[])\n",
    "      convo.send_message(text)\n",
    "      ko_text = convo.last.text\n",
    "    except:\n",
    "      try:\n",
    "        print(f\"1st fail. ({i})\")\n",
    "        sleep(second_sleep)\n",
    "        convo = model.start_chat(history=[])\n",
    "        convo.send_message(text)\n",
    "        ko_text = convo.last.text\n",
    "      except:\n",
    "        print(f\"2nd fail. Pass ({i})\")\n",
    "        ko_text = ''\n",
    "    if len(text.split(\"<sep>\")) == len(ko_text.split(\"<sep>\")):\n",
    "      train_dataset_ko_df.loc[i, 'korean_conversations'] = ko_text\n",
    "      train_dataset_ko_df.to_csv(file_path, index=False)\n",
    "      print(f\"({i})\\n{ko_text}\")\n",
    "    else:\n",
    "      print(f\"({i}) ### Something's wrong!!! ###\\n{ko_text}\")\n",
    "    sleep(first_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ko_lima(train_dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ko_lima(train_dataset_df, resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ko_lima_df = pd.read_csv('./data/ko_lima.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge(en:str, ko:str):\n",
    "  en_lst = [c.strip() for c in en.split('[sep]')]\n",
    "  ko_lst = [c.strip() for c in ko.split('[sep]')]\n",
    "\n",
    "  assert len(en_lst) == len(ko_lst)\n",
    "  n = len(en_lst)\n",
    "  en_ko = '[sep]'.join([ko_lst[i] if i%2 else en_lst[i] for i in range(n)])\n",
    "  ko_en = '[sep]'.join([en_lst[i] if i%2 else ko_lst[i] for i in range(n)])\n",
    "\n",
    "  return en_ko, ko_en\n",
    "\n",
    "ko_lima_df.loc[:, ['en_ko', 'ko_en']] = ko_lima_df.loc[:, ['conversations', 'korean_conversations']].apply(_merge, axis=1)\n",
    "\n",
    "ko_lima_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_lima_df.to_csv('./data/bi_lima.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
